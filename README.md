# Imitation Learning (Expert ~ Heuristics)

Imitation Learning, or behavior cloning, is an artificial intelligence techinique that relies on supevervised-learning to address sequential decision making problems. 

The purpose of this repository is to demontrate imitation learning on a simple continuous control problem: Open AI LunarLanderContinuous-v2.

# Algorithm structure
- Demonstration data obtained from environment built-in "heuristics"
-- Control function based on position and speed

- Policy:   
            
            Linear(in_features=8, out_features=64, bias=True),
            
            ReLU(),
            
            Linear(in_features=64, out_features=64, bias=True),
            
            ReLU(),
            
            Linear(in_features=64, out_features=2, bias=True)   

- Supervised learning is applied (MSELoss for an adjustable number of epochs)

# Empirical results
1. When only one demonstration is available, generalization may be substantially affected: 

1a) One demonstration (Score 300) lead to 40% victory rate
![TrainingPerf_bc_LunarLanderContinuous-v2_nepochs=10000_lr=0 0001_n_traj=1_](https://user-images.githubusercontent.com/86380991/123563759-c676cd80-d78c-11eb-8fb3-91e13a3eff26.png)
![Evaluation_bc_LunarLanderContinuous-v2_nepochs=10000_lr=0 0001_n_traj=1_](https://user-images.githubusercontent.com/86380991/123564548-32a70080-d790-11eb-84a4-69cfe9ba3d5b.png)

1b) One demonsration (Score 315) lead to 50% victory rate
![TrainingPerf_bc_LunarLanderContinuous-v2_nepochs=10000_lr=0 0001_n_traj=1_](https://user-images.githubusercontent.com/86380991/123565547-adbde600-d793-11eb-96ee-db10b2dd37b1.png)
![Evaluation_bc_LunarLanderContinuous-v2_nepochs=10000_lr=0 0001_n_traj=1_](https://user-images.githubusercontent.com/86380991/123565554-b0b8d680-d793-11eb-89ac-eb9166178b32.png)

1c) One demonstration (Score 281) lead to 99.1% victory rate
![TrainingPerf_bc_LunarLanderContinuous-v2_nepochs=10000_lr=0 0001_n_traj=1_](https://user-images.githubusercontent.com/86380991/123565354-f88b2e00-d792-11eb-9aa0-9ff27abcbd03.png)
![Evaluation_bc_LunarLanderContinuous-v2_nepochs=10000_lr=0 0001_n_traj=1_](https://user-images.githubusercontent.com/86380991/123565359-faed8800-d792-11eb-93fd-9c5bbe709439.png)




2. With 3 trajectories, results are more stable, even with reduced # of epocs:

2a) Three demonstrations (Scores: 292, 299, 305) leads to 88.1% victory rate
![TrainingPerf_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=3_](https://user-images.githubusercontent.com/86380991/123566011-080b7680-d795-11eb-9974-52ee9cd3b9fe.png)
![Evaluation_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=3_](https://user-images.githubusercontent.com/86380991/123566020-0b066700-d795-11eb-83c4-9851f5970a08.png)

2b) Three demonstrations  lead to 97.6% victory rate
![TrainingPerf_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=3_](https://user-images.githubusercontent.com/86380991/123566391-fbd3e900-d795-11eb-89c4-647848ec3739.png)
![Evaluation_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=3_](https://user-images.githubusercontent.com/86380991/123566942-620d3b80-d797-11eb-9b28-44453984b0f9.png)


3. With 10 trajectories, results consistently get close or at perfection:

3Ã ) Ten demonstrations ==> 99%
![TrainingPerf_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=10_](https://user-images.githubusercontent.com/86380991/123569385-9800ee80-d79c-11eb-829c-9d257b112ed2.png)
![Evaluation_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=10_](https://user-images.githubusercontent.com/86380991/123569392-9afbdf00-d79c-11eb-864c-9f2096fbf4f3.png)


3b) Ten demonstrations ==> 98.9
![TrainingPerf_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=10_](https://user-images.githubusercontent.com/86380991/123570040-d77c0a80-d79d-11eb-8519-ad253aff8486.png)
![Evaluation_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=10_](https://user-images.githubusercontent.com/86380991/123570044-d945ce00-d79d-11eb-962f-4765637baad3.png)


3c) Ten demonstrations ==> 100%
![TrainingPerf_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=10_](https://user-images.githubusercontent.com/86380991/123569174-32146700-d79c-11eb-8385-56ed7d6f71fe.png)
![Evaluation_bc_LunarLanderContinuous-v2_nepochs=3000_lr=0 0001_n_traj=10_](https://user-images.githubusercontent.com/86380991/123569172-317bd080-d79c-11eb-908d-e44a15d5ef82.png)


# Conclusion

Given demonstrations generated by a well defined control function ('heuristics'), pure behavior cloning is able to achieve ~99% sucess rate for LunarLanderContinuous-v2 on test environment.



